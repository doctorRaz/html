---
title: Фильтрация треков GPS на F#, Часть III
id: fsharp-gps-tracks-filtration-2
excerpt: Фрагменты лекции, прочитанной в Московском клубе программистов 21 февраля 2019 года.
mathjax: true
---

## Фильтр Калмана

Мы добрались до самого интересного&nbsp;&mdash; сглаживания треков. Программа, которую мы напишем, окажется простой и короткой, но за этой простотой скрывается нетривиальная математика.

Чтобы не запутаться, будем рассуждать последовательно.

### Модель

В чём задача сглаживания? На автомобиле установлен датчик GPS, который определяет координаты каждые несколько секунд. Показания датчика не очень точные, поэтому трек получается неровным.

Чтобы его *выровнять*, нам нужно что-то, что позволит исправлять ошибки датчика. В методе Калмана это *что-то*&nbsp;&mdash; математическая модель.

Рассмотрим утрированно простой вариант движения автомобиля&nbsp;&mdash; представим, что он передвигается в одномерном пространстве.

![Модель движения автомобиля](/img/kalman-1.png){: .picture}

Автомобиль едет по прямой дороге. Время от времени мы определяем его положение и скорость. На шаге $i$ координата автомобиля равна $x_i$, а скорость равна $u_i$. На шаге $i + 1$, через время $\Delta t$, новая координата будет равна $x_{i+1}$.

$$
x_{i+1} = x_i + u_i \Delta t
$$

Это и есть модель движения автомобиля. Мы видим, что она простая, и не учитывает, например, *ускорения*, поэтому её предсказания будут обладать *погрешностью*.

Математически идею погрешности можно выразить, добавив в равенство случайную величину. Математики традиционно используют для обозначения величин самые странные буквы древних алфавитов. В теории вероятностей особой любовью пользуется греческий алфавит, поэтому обозначим погрешность буквой *кси*, $\xi$.

$$
x_{i+1} = x_i + u_i \Delta t + \xi_i
$$

Индекс $i$ у $\xi$ показывает, что погрешность в каждой точке разная. Действительно ли она случайна? Мы не знаем. Наверное мы могли бы сделать модель сложнее, и вычислять погрешность с большей точностью, но у нас нет задачи строить сложные модели.

На практике не важно, действительно ли погрешность случайна или она только *похожа* на случайную&nbsp;&mdash; мы в любом случае можем применить к ней математический аппарат.

### Вспоминая теорию вероятностей

Поскольку погрешность это *непрерывная случайная величина*, при каждом измерении мы будем получать новое значение, которое невозможно предсказать заранее. Но, проведя множество измерений, мы сможем утверждать о случайной величине кое-что определённое.

![Случайная величина](/img/kalman-2.png){: .picture}

Взгляните на иллюстрацию. Я свёл две случайные величины, которые объединяет одна характеристика. Вы видите, что значения во всех испытаниях сосредоточены вокруг единого центра. Вычислив *среднее арифметическое* всех величин, узнаем значение, которое *ожидаем* обычно. Оно так и называется&nbsp;&mdash; математическое ожидание&nbsp;&mdash; и обозначается буквой $E$&nbsp;&mdash; первой в слове *Expectation*.

$$
E \xi = \frac{\xi_1 + \xi_2 + ... + \xi n}{n}
$$

На иллюстрации математическое ожидание $E \xi$ у обеих величины равно 2.

Вторая характеристика случайной величины&nbsp;&mdash; разброс. В левой половине графика значения находятся чуть дальше от центра, а в правой&nbsp;&mdash; чуть ближе. Разброс показывает, как далеко случайная величина $\xi$ может отстоять от центра $E \xi$. Проблема в том, что разница $\xi - E \xi$ бывает как отрицательной, так и положительной. Чтобы всегда получать положительные значения, возведём её в квадрат $(\xi - E \xi)^2$.

Однако, в выборке много случайных величин. Какую из них взять, чтобы оценить разброс? На рисунке мы видим, что максимальная разница для двух выборок равна 1, в то время как &laquo;на глазок&raquo; левая выборка шире правой.

Чтобы точнее оценивать разброс, вместо максимальной разницы надо брать среднюю, то есть $E(\xi - E \xi)^2$.

Эта мера разброса называется *дисперсией*.

$$
\sigma^2_\xi = E(\xi - E \xi)^2
$$

Наша погрешность измеряется в метрах, а дисперсия&nbsp;&mdash; в метрах-в-квадрате. Это не всегда удобно, поэтому помимо дисперсии в теории вероятностей оперируют *стандартным отклонением*.

$$
\sigma_\xi = \sqrt{E(\xi - E \xi)^2}
$$

Стандартное отклонение в левой выборке равно 1, а в правой&nbsp;&mdash; 0,6. В отличие от отдельных случайных величин, математическое ожидание, дисперсия и стандартное отклонение вполне предсказуемы. Если мы возьмём сто случайных чисел, посчитаем среднее, а потом возьмём ещё сто случайных чисел, и снова посчитаем среднее, эти два средних будут равны друг другу с высокой степенью точности.

Но вернёмся к погрешности модели. Чему может быть равно её математическое ожидание? Модель иногда даёт перекос в большую сторону, иногда в меньшую, но в среднем точно оценивает пройденное расстояние. Это значит, что среднее арифметическое погрешности должно быть равно нулю, $E \xi = 0$.

Если это так, то дисперсия погрешности равна

$$
\sigma^2_\xi = E(\xi - E \xi)^2 = E(\xi - 0)^2 = E \xi^2
$$

### Датчик

Датчик GPS тоже обладает погрешностью. По сложившейся традиции, обозначим её греческой буквой с необычной внешностью, например, буквой *эта*.

$$
x^S_i = x_i + \eta_i
$$

$x^S_i$ означает не возвещение в степень $S$, а величину $x_i$ помеченую буквой $S$, первой буквой слова *Sensor*. Так мы обозначим координату, полученную с датчика.

Погрешность датчика может быть и положительной, и отрицательной. В откалиброванном датчике математическое ожидание погрешности будет равно нулю, $E \eta = 0$, а дисперсия

$$
\sigma^2_\eta = E(\eta - E \eta)^2 = E(\eta - 0)^2 = E \eta^2.
$$

### Рекурсивный метод

Фильтр Калмана работает *рекурсивно*. Предположим, что нам известна координата $x^A_i$, вычисленная на предыдущем шаге алгоритма, и мы хотим вычислить следующую координату $x^A_{i+1}$. Буква $A$&nbsp;&mdash; первая в слове *Appoximation*, то есть *приближение*.

Нам известны скорость $u^S_i$, которую датчик GPS определил на предыдущем шаге, и текущая координата, полученная с датчика сейчас $x^S_{i+1}$.

Новое приближение будет находиться где-то между координатой датчика $x^S_{i+1}$ и координатой, вычисленной по модели, т. е. $x^A_i + u^S_i \Delta t$.

Чтобы найти это *между*, воспользуемся простой *линейной интерполяцией*.

$$
x^A_{i+1} = K x^S_{i+1} + (1 - K)(x^A_i + u^S_i \Delta t)
$$

Коэффициент $K$ определяет относительные &laquo;веса&raquo; датчика и модели. При $K = 1$ координата определяется только датчиком, а при $K = 0$&nbsp;&mdash; только моделью.

*Ошибкой предсказания* назовём разницу между реальной координатой $x_{i+1}$ и предсказанной $x^A_{i+1}$.

$$
e_{i+1} = x_{i+1} - x^A_{i+1}
$$

Так как $x^A_{i+1} = K x^S_{i+1} + (1 - K)(x^A_i + u^S_i \Delta t)$,

$$
e_{i+1} = x_{i+1} - K x^S_{i+1} - (1 - K)(x^A_i + u^S_i \Delta t)
$$

Поскольку $x^S_i = x_i + \eta_i$ (и $x^S_{i+1} = x_{i+1} + \eta_{i+1}$), то

$$
e_{i+1} = x_{i+1} - K (x_{i+1} + \eta_{i+1}) - (1 - K)(x^A_i + u^S_i \Delta t)
$$

Вспоминаем, что $x_{i+1} = x_i + u^S_i \Delta t + \xi_i$, тогда

$$
e_{i+1} = x_i + u^S_i \Delta t + \xi_i - K (x_i + u^S_i \Delta t + \xi_i + \eta_{i+1}) - (1 - K)(x^A_i + u^S_i \Delta t)
$$

Вынесем $x_i + u^S_i \Delta t + \xi_i$ за скобки, получим

$$
e_{i+1} = (1 - K)(x_i + u^S_i \Delta t + \xi_i) - K \eta_{i+1} - (1 - K)(x^A_i + u^S_i \Delta t)
$$

Видим, что множитель $(1 - K)$ дважды встречается в выражении, и, следовательно, его можно вынести. При этом у нас сократится член $u^S_i \Delta t$

$$
e_{i+1} = (1 - K)(x_i - x^A_i + \xi_i) - K \eta_{i+1}
$$

Зная, что $e_{i+1} = x_{i+1} - x^A_{i+1}$, то есть $e_i = x_i - x^A_i$, сокращаем запись во вторых скобках

$$
e_{i+1} = (1 - K)(e_i + \xi_i) - K \eta_{i+1}
$$

### Магия

Мы хотим минимизировать ошибку $e_{i+1}$. Как это сделать? Первый ответ, который приходит в голову&nbsp;&mdash; никак. Ошибка&nbsp;&mdash; случайная величина, значения которой мы не знаем.

Тупик.

Математики, оказавшись в тупике, смотрят на задачу с непривычных точек зрения. Да, мы не знаем, чему равна $e_{i+1}$, но предположим, что мы много раз поставили эксперимент с автомобилем. Тогда мы сможем посчитать математическое ожидание ошибки $E e_{i+1}$&nbsp;&mdash; оно постоянно, и его можно минимизировать.

Звучит интересно. Попробуем заменить все случайные величины их ожиданиями

$$
E e_{i+1} = (1 - K)(E e_i + E \xi_i) - K E \eta_{i+1}
$$

Как минимизировать такую ошибку? Непонятно. Идея Калмана кажется совершенно парадоксальной&nbsp;&mdash; мы должны вычислять не ожидание ошибки, а квадрат ожидания $E e^2_{i+1}$.

Не похоже на простое решение, не так ли?

### Проверка

$$
E e^2_{i+1} = ((1 - K)(E e_i + E \xi_i) - K E \eta_{i+1})^2
$$

Раскрываем скобки и ужасаемся

$$
E e^2_{i+1} = (1 - K)^2 (E e_i + E \xi_i)^2 - 2(1 - K)(E e_i + E \xi_i) K E \eta_{i+1} + K^2 E \eta^2_{i+1}
$$

Мы вспоминаем, что наш датчик хорошо откалиброван, и, следовательно, в среднем даёт ошибку ноль, то есть $E \eta{i+1} = 0$. Следовательно, всё произведение, куда входит этот множитель, тоже равно нолю

$$
E e^2_{i+1} = (1 - K)^2 (E e_i + E \xi_i)^2 + K^2 E \eta^2_{i+1}
$$

Возводим $E e_i + E \xi_i$ в квадрат

$$
E e^2_{i+1} = (1 - K)^2 (E e^2_i + 2 E e_i E \xi_i + E \xi^2_i) + K^2 E \eta^2_{i+1}
$$

Самое время вспомнить, что наша модель в среднем также даёт ошибку ноль, $E \xi_i = 0$

$$
E e^2_{i+1} = (1 - K)^2 (E e^2_i + E \xi^2_i) + K^2 E \eta^2_{i+1}
$$

Чему равно значение $E \xi^2_i$? Кажется странным, что в каждой точке трека модель имеет рызную погрешность. Действительно, ожидания погрешности должны быть равны во всех точках $E \xi^2_i = E \xi^2$. Провернём небольшой трюк, чуть усложнив формулу $E \xi^2 = E(\xi - 0)^2$.

Вспоминаем, что $E \xi = 0$. Подставим ожидание вместо нуля&nbsp;&mdash; $E(\xi - E \xi)^2$. Это формула расчёта дисперсии $\sigma^2_{\xi}$.

Те же соображения касаются $E \eta^2_{i+1}$. Следовательно

$$
E e^2_{i+1} = (1 - K)^2 (E e^2_i + \sigma^2_{\xi}) + K^2 \sigma^2_{\eta}
$$

### Новое чудо

Мы получили следующую формулу, которая всё ещё не позволяет ответить на вопрос, как минимизировать ошибку. Но конец уже близок, потерпите.

