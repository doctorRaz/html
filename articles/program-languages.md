---
title: Языки программирования
excerpt: Сакрально-ироническое введение в историю и перспективы.
id: program-languages
---

### Кризис оснований математики

В основу современного программирования легли несколько математических работ, написанных в 20&ndash;30-х годах XX века. Наша профессия возникла как побочный
эффект битвы математиков за математику.

Битва эта началась в конце XIX века, когда разные части математики удалось свести в единую систему. С помощью *теории множеств* учёные обосновали и алгебру, и геометрию, и исчисление предикатов.

Недолгую идилию разрушил Бертран Рассел. Он сформулировал парадокс, доказавший противоречивость теории множеств. И, поскольку из неё следует
даже элементарная арифметика, перед математиками встал вопрос, не противоречива ли и вся математика?

> #### Парадокс Рассела
> Предположим, у нас есть множество всех множеств, которые не являются собственными элементами. Является ли это множество собственным элементом?
> * Если да, то оно не может входить в множество всех множеств, которые не являются собственными элементами, и, таким образом, не является собственным элементом.
> * Если нет, то оно входит в множество всех множеств, которые не являются собственными элементами, и, таким образом, является собственным элементом.
> Формулировка выглядит шутливой, однако именно парадокс Рассела привёл к *кризису оснований математики*.

Причина парадокса в том, что классическая (наивная) теория множеств не слишком формально определяет *множества всех множеств* и
*множества, которые являются собственными элементами*. Математики, в том числе и Рассел, предложили несколько способов решения проблемы, и, в конце концов,
теория устояла.

Однако новый неожиданный вопрос так и остался без ответа: как доказать непротиворечивость математики?

> #### Непротиворечивость
> Стоит ли вообще городить огород? Так ли плоха противоречивость? И, кстати, что это такое?
>
> Противоречивость это свойство теории. Теорию называют противоречивой, если в ней истинны и утверждение, и его отрицание. Математически это можно записать как
> **(p&nbsp;&amp; &not;p)&nbsp;&rArr; q**. Из утверждения **p** и его отрицания **&not;p** следует утверждение **q**. По закону исключения третьего **(p&nbsp;&amp; &not;p)**
> всегда **false**, так что мы можем записать **false&nbsp;&rArr; q**.
>
> По правилам **импликации** из ложной посылки могут следовать как истинные, так и ложные следствия, то есть **q** может быть любым. Иначе говоря, если теория
> противоречива, из неё можно вывести всё, что угодно.
>
> Практическая польза теорий заключается в том, что они позволяют делать достоверные предсказания. Если теория предсказывает противоположные исходы, практически она бесполезна.

За доказательство непротиворечивости взялся *предводитель математиков* Давид Гильберт. Он формализовал понятия *теории* и *доказательства*, и предложил исследовать их математическими средствами.

Учёным удалось формально описать элементарную логику, логику предикатов, арифметику и теорию множеств. Каждую из этих теорий можно представить, как расширение предыдущей, например,
исчисление предикатов это исчисление высказываний с *предикатами* и *кванторами*, а арифметика, в свою очередь&nbsp;&mdash; исчисление предикатов с *нулём*, операцией *следующе число*,
и операцией *сложение*.

Будучи авторами теории, мы могли бы задаться такими вопросами:

1. Достаточно ли у нас аксиом для того чтобы выести все важные теоремы? *Полон* ли набор аксиом?

1. Нет ли среди аксиом тех, которые дублируют друг друга? *Независимы* ли аксиомы?

1. Не противоречат ли правила вывода друг другу? *Непротиворечива* ли теория?

1. Какова сложность теории? Можно ли предложить способ поиска доказательств? Если нет, можно ли *разрешить вопрос*, является теорема доказумой, или нет?

Для самой простой из теорий&nbsp;&mdash; исчисления высказываний&nbsp;&mdash; можно доказать и полноту, и непротиворечивость. Алгоритм *разрешения* в элементарной
логике также существует. Например, если мы хотим доказать *закон контрапозиции* (*x* &rArr; *y*) &rArr; (&not;*y* &rArr; &not;*x*), нам достаточно
построить таблицу истинности:

| *x* | *y* | *u* = *x* &rArr; *y* | *v* = &not;*y* &rArr; &not;*x* | *u* &rArr; *v* |
|-----|-----|-------------------------------------------------------|----------------|
|  0  |  0  |                    1 |                              1 |              1 |
|  0  |  1  |                    1 |                              1 |              1 |
|  1  |  0  |                    0 |                              0 |              1 |
|  1  |  1  |                    1 |                              1 |              1 |

Поскольку в крайней правой колонке всегда 1, теорема истинна, хотя мы и не можем предложить метода доказательства, кроме полного перебора.

Следующая по сложности теория&nbsp;&mdash; исчисление предикатов&nbsp;&mdash; также полна и непротиворечива, как и арифметика с операцией сложения.

А вот арифметика со сложением и умножением уже принципиально неполна, независимо от количества аксиом. Неполны и все теории, которые включают арифметику, например, теория множеств.

Это положение доказал Курт Гёдель. Его первая теорема о неполноте гласит, что любая формальная теория, не менее сложная, чем арифметика, либо полна, либо непротиворечива. Вторая теорема утверждает,
что средствами арифметики нельзя доказать её собственную непротиворечивость.

Следует ли из этого, что мы доказали противоречивость арифметики? Нет. Ученик Гильберта немецкий математик Генцен, доказал непротиворечивость арифметики, применив более мощную теорию.
Но её непротиворечивость также требовала обоснования. В полном соответствии с теоремами Гёделя, надеждам на обоснование математики не суждено было сбыться.

Но, если математики проиграли сражение за полноту и непротиворечивость, может быть они могли выиграть на поле разрешимости?

### Проблема разрешения

Как мы помним, в исчеслении высказываний существует *разрешаюшая процедура* для определения истинности теорем. Чтобы узнать, верна ли теорема, мы строим таблицу истинности и изучаем
столбец результата. Жизнь математиков сильно упростилась бы, умей они определять истинность теорем во всех системах.

Уже известный нам Давид Гильберт сформулировал *проблему разрешения*&nbsp;&mdash; найти *разрешающую процедуру* для систем сложнее логики.

*Проблема разрешения* (*нем.* Entscheidungsproblem, *произносится* ɛntˈʃʌɪdʊŋsˌpɹɒbləm) важна тем, что вводит понятие *процедуры* или *алгоритма*. Чтобы разобраться с ней,
американский математик Алонзо Чёрч применил разработанное им лябда-исчисление.

> #### Лямбда-исчисление
> Чёрч исследовал природу функций, и с помощью лямбда-записи формализовал такие понятия, как **функция**, **параметр**, **применение
> функции** и **связывание переменной**. Процесс **вычисления** у Чёрча не фиксирован. Мы можем взять функцию **λx.xx**, применить её
> к **m** и получить в результате **mm**, никак не поясняя это означает задвоение&nbsp;&mdash; мы просто подставили **m** вместо **x**.
>
> Что можно сделать с помощью такой несложной абстракции? Чёрчу удалось представить в виде λ-функций логику и арифметику
> натуральных чисел вместе с операциями конъюнкции, дизъюнкции, сложения и умножения (ищите в сети **булеаны Чёрча** и **нумералы Чёрча**).
>
> **Вычислимыми** считаются функции, которые можно представить в виде λ-выражения, например, сложение.

Вычислима ли разрешающая процедура для логики предикатов? В своей работе Чёрч доказал, что нет. Следовательно, *проблема разрешения*
не может быть решена ни для неё, ни для более сложных систем.

С другой стороны, некоторые функции вполне себе λ-определимы, и, следовательно, у нас есть способ их вычисления.

Через несколько месяцев после Чёрча неразрешимость Entscheidungsproblem доказал английский математик Алан Тьюринг, использовав умозрительную конструкцию, которая сейчас называется *машиной Тьюринга*.

> #### Машина Тьюринга
> Машина похожа на обычную печатную машинку, правда, количество кареток в ней не ограничено. Каждая каретка работает со своей лентой, которая бесконечна в обе стороны.
>
> Машина умеет выполнять команды: напечать символ, стереть символ, сдвинуть каретку вправо или влево. В &laquo;программе&raquo; Тьюринга все команды пронумерованы,
> но концепции &laquo;последовательного исполнения&raquo; там нет&nbsp;&mdash; в конце каждой команды надо указать, какую команду выполнить следующей.

Что такое вычислимость по Тьюрингу? Возьмём машину из трёх лент и на первых двух лентах запишем два числа в унарной системе счисления.
Единицу обозначим символом **X**, двойку&nbsp;&mdash; символами **XX** в двух соседних клетках, тройку&nbsp;&mdash; **XXX**, и так далее. Последнюю ленту оставим пустой.

Сможем ли мы составить программу, которая напечатает на третьей ленте сумму чисел с первых двух лент? Если нам это удастся (подсказка: а нам удастся), то сложение
*вычислимо по Тьюрингу*.

Тьюринг доказал, что нельзя составить разрешающую процедуру для арифметики, как программу для его машины. Так как его работа появилась позже работы Чёрча, он
сравнил два доказательства. Оказалось, что с помощью λ-исчисления и машины Тьюринга можно вычислять одни и те же функции.
Как сказал бы математик, классы вычислимых функций совпадают. Для программиста это означает, что любую функциональную программу, можно 
написать на императивном языке, а любую императивную&nbsp;&mdash; на функциональном.

### Толстые и худые языки программирования

Если λ-исчисление и машина Тьюринга эквиваленты, означает ли это, что экивалентны все известные *модели вычисления*? Нет.

Языки, не менее мощные, чем машина Тьюрига, называются полными по Тьюрингу. Существуют и неполные языки, например, *регулярные выражения*.
Предположим, мы хотим написать программу, которая проверяет правильность арифметической формулы. Одним из правил для проверки
будет парность скобок. В регулярных выражениях не существует способа проверить, что количество левых скобок равно количеству правых, кроме
как явно перечислить все случаи:

    ε | (ε) | ((ε)) | (((ε))) | …

Для бесконечной формулы нам потребуется написать бесконечное регулярное выражение, в то время как *контекство-свободная грамматика*
позволяет обойтись короткой программой:

    ε ::= (ε)

Если говорить об эквивалентности, то регулярные выражения совпадают с конечными автоматами, а контекстно-свободные грамматики&nbsp;&mdash; со
стековой машиной (конечный автомат с одним стеком).

Но и стековая машина не является Тьюринг-полной. Предположим, мы считаем правильными такие строки:

    αβγ
    ααββγγ
    αααβββγγγ
    α…αβ…βγ…γ
     ͫ  ͫ  ͫ

Количество символов `α`, `β` и `γ` должно совпадать. Стековая машина, в отличие от привычных языков программирования,
не позволяет распознавать такое множество строк. Но стоит добавить к ней второй стек, как она становится эквивалентна машине Тьюринга.

На практике, от языков программирования не требуется быть Тьюринг-полными. Например, один из самых популярных&nbsp;&mdash; C&nbsp;&mdash; не
является полным из-за *адресной арифметики*. Указатель в C может быть преобразован к целому числу, а это означает, что память C-машины потенциально
ограничена. В то же самое время, в Java и C# указатель это просто указатель, стандарты языков не накладывают ограничений на его размер. Используя
потенциально бесконечную структуру данных *однонапралвенный список*, мы можем описать объект любого размера. А вот массивы в Java и C# имеют
целочисленный размер, поэтому не могут являться основой для Тьюринг-полной программы.

Мы можем делать выводы об языке программирования, зная, является ли он Тьюринг-полным. Скажем, классический SQL не поддерживает рекурсию,
поэтому не позволяет извлекать иерархические данные произвольной вложенности. С другой стороны, SQL с поддержкой обобщённых
табличных выражений (common table expressions)&nbsp;&mdash; позволяет.

Препроцессор C не реализует циклы, поэтому с его помощью нельзя вычислять числа Фибоначчи на этапе компиляции. А с помьщью Тьюринг-полных шаблонов C++&nbsp;&mdash; можно.

Модели вычисления играют важную роль, если в расчёт принимаются временная сложность алгоритма и простота его реализации: примитивные вычислители и проще в разработке, и быстрее.
Именно поэтому для создания компиляторов в UNIX используют две программы: **Lex** и **Yacc**. Первая умеет расознавать лексемы и по сложности соответствует конечному автомату.
Вторая сложнее, она разбирает бесконечно вложенные выражения и соответствует стековой машине. Мы могли бы ограничиться одной программой **Yacc**, поскольку она умеет то же, что
и **Lex**. Но раз стековая машина работает медленнее, компилятор, сделанный только на **Yacc**, тоже окажется медленнее.

Все программисты знают, как иногда мучительны медленные компиляторы.

### Первые языки программирования

Историю языков программирования можно разложить в простую схему, которая, как и все простые схемы, будет неверное. Например, можно сказать, что первая веха в становлении
программирования&nbsp;&mdash; период с 45-го по 55-й год. Удобно, что числа круглые.

На самом деле, писать первые программы начали чуть раньше 45-го года, в 43-м. Средством общения с компьютером были *машинные коды*. *Язык ассемблера* появился в 49-м году.
Веха закончилась созданием первого высокоуровневого языка программирования *Фортран* в 54-м году.

«Веха 43&ndash;54» выглядит совершенно некругло. И год окончания не совсем корректен&nbsp;&mdash; на языке ассемблера массово писали даже в 80-х, так что появление Фортрана,
если и изменило ситуацию, то не сразу, и не быстро. Так что говорить мы будем не о вехах, а о тенденциях.

Архитектура первых компьютеров была основана на модели Тьюринга. Собрать машину из электронных компонентов того времени оказалось проще, чем λ-вычислитель.
Аналогом ленты в компьютере служит оперативная память. Она состоит из ячеек, куда можно записать число из небольшого диапазона, или, как сказали бы
математики, символ из ограниченного алфавита. В отличие от ленты, память позволяет обращаться к прозвольной ячейке напрямую.

Первую память делали десятичной, и, как результат, достаточно сложной. Программы кодировались перемечками на специальной панели, что было неудобно.
Уже к 46-му году разработчики придумали, как упростить схему вычислительных машин. Набор принципов был изложен Джоном фон Нейманом, и известен как *архитектура фон Неймана*:

1. В одной и той же памяти хранится как данные, так и команды.
1. Память компьютера пронумерована, номер или адрес ячейки позволяет процессору прочитать или изменить содержимое ячейки.
1. Процессор интерпретирует программу, как набор команд в оперативной памяти. Он выполняет их последовательно, пока не встретит команду перехода.
1. Вся информация кодируется в двоичной системе.

Машинный код&nbsp;&mdash; программа в том виде, как её видит процессор, то есть набор двоичных слов, каждое из которых либо *код команды*, либо её *параметры*.
Такая программа *императивна*, поскольку содержит набор инструкций (указаний, императивов), которые машине следует выполнить.

Первым языком, упростившим работу программистов, стал язык Ассемблера. От машинного кода он отличается тем, что вместо кодирования команд числами,
позволял записывать их в мнемонической форме. По суть это тот же машинный код, записанный чуть более понятно.

Новый язык программирования&nbsp;&mdash; Фортран&nbsp;&mdash; гораздо сильнее упростил работу программистов. Его главное преимущество раскрывается в названии&nbsp;&mdash; Formula Translator.
Фортран научился делать одну простую, но очень важную работу&nbsp;&mdash; переводить выражения вида `SQRT(X + Y * 38.7)` в последовательность машинных команд.
Другие отличия от языка ассемблера были незначительным, даже условный оператор в его привычной форме `if…then…else` в Фортране отсутствовал.
Вместо него программист ставил метки, управление на которые передавалось, если выражение оказывалось меньше, больше, либо равно нулю.

Отмечу ещё два важных нововведения. Во-первых, это оператор цикла DO, который не имел аналога в языке ассемблера. Эта конструкция «собиралась» из IF и GOTO, то есть, по сути, являлась
синтаксическим сахаром. Через 15 лет (почти через 15) она позволила сформулировать принципы *структурного программирования*.

Во-вторых, это стандартизация вызова процедур. Программируя на машинном языке, мы можем вызывать подпрограммы, самостоятельно организуя передачу параметров и возврат значений через регистры,
стек, кучу и глобальную память. Даже в пределах одной программы допустимо произвольным образом смешивать разные способы вызова. Однако появление компилятора привело к необходимости формализовать
*соглашение о вызовах*. Если разные компиляторы следуют одному соглашению, можно стыковать подпрограммы, написанные на разных языках, благодаря чему код на Фортране до сих пор легко подключить
к вашим проектам, написанным на C, C++ и даже Java.

Не смотря на то, что Фортран является императивным языком, запись математических выражений на нём функциональна. Увидев формулу `A * B + A * C` мы не можем предсказать, в каком порядке
будут вычисляться слагаемые: сначала `A * B` или сначала `A * C`. Истинно императивной записью выражений является постфиксная форма, которая обязывает программиста думать о порядке
выполнения операций. `A B * A C * +`&nbsp;&mdash; здесь, как видим, разночтений нет.

Программа вычисления первых двадцати чисел Фибоначчи на Фортране. Используем современную версия языка, потому что на классику существующие компиляторы ругаются.

```fortran
program fibonacci
  integer a, b, t
  a = 0
  b = 1
   
  do n = 1,20
    print *, a
    t = a
    a = a + b
    b = t
  end do
end program fibonacci
```

Мы видим аккумуляторы `a` и `b`, которые хранят два последних вычисленных числа.
Временная переменная `t` позволят обменивать значения `a` и `b` перед тем, как вычислить следующее число. Благордаря этому обмену мы сохраняем
семантику &laquo;`a`&nbsp;&mdash; предпоследнее вычисленное число, а `b`&nbsp;&mdash; последнее&raquo; Мы могли бы отказаться от переменной `t`,
но код станет непонятным:

```fortran
    print *, a
    a = a + b
    b = a - b
```
Результат работы:

```
           0
           1
           1
           2
           3
           5
           8
          13
          21
          34
          55
          89
         144
         233
         377
         610
         987
        1597
        2584
        4181
```

Функциональные языки появились почти одновременно с императивными. LISP, дедушка современного функционального программирования, был разработан в 1958-м году на основе
λ-исчисления. Программа печати первых двадцати чисел, написанная на современной версии языка, Common Lisp:

```common-lisp
(defun fibonacci-series (n)
    (labels ((fibonacci-rec (m a b)
         (cond
             ( (= m 1) (cons a '()))
             (t (cons a (fibonacci-rec (- m 1) b (+ a b)))))))
     (fibonacci-rec n 0 1)))

(print (fibonacci-series 20))
```

Результат:

```
(0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181)
```

Мы описываем функцию, которая строит список чисел Фибоначчи, состоящий из заданного числа элементов. Эта функция рекурсивна, и использует так называемые
*накапливающие параметры* `a` и `b`. При первом вызове их значения должны быть равны `0` и `1` и, в идеале, их надо спрятать от вызывающего кода.
Для это сделать, поместим реальную вычисляющую функцию `fibonacci-rec` внутрь `fibonacci-series`.

## Структурное программирование

Конец эпохи Фортрана принято связывать со знаменитой статьёй Дейкстры «О вреде оператора GOTO», которая вышла в 1968г. В данном случае слова «конец эпохи» следует воспринимать условно,
поскольку в математических программах Фортран активно используется до сих пор.

В действительности основы структурного подхода были заложены за десять лет до статьи, при разработке языка программирования Algol-58. Если Fortran следовал той же парадигме
программирования, что и машинный код&nbsp;&mdash; программа это просто набор команд, то в Algol ввели понятие *структуры*. Программа разбивалась на *блоки*, которые могли быть вложенными.
Плюс такого разбиения заключается в снижении количества ошибок. При программировании на Fortran бичом программистов был так называемый *спагетти-код*, в котором сложно отследить порядок
выполнения, поскольку управление постоянно скачет то вверх, то вниз.

Дейкстра предложил несколько принципов, которые позволили читать подпрограмму сверху вниз. Он доказал, что любой спагетти-код можно преобразовать в последовательность одиночных операторов,
циклов и ветвлений. В современных языках оператор GOTO практически не встречается, вместо него используют такие операторы как `break`, `continue` и `return`.

Структура программ позволила разрабатывать средства для работы с *потоком управления*. Например, компилятор может предупредить программиста о *недостежимом коде*, потому что знает,
что в определённые места программы управление никогда не попадёт.

## Типы данных

Fortran умел работать с переменными двух типов: целых и действительных чисел. Переменные можно было не описывать, поскольку в языке применялись правила автоматического объявления типа.
Переменные, чьё имя начиналось на `I`, `J`, `K`, `L`, `M` и `N` считались целыми, все прочие&nbsp;&mdash; действительными.

При разработке Algol объявление переменных вместе с указанием их типа сделали явным. В программе необходимо указать название каждой используемой переменной и её тип. Такое дублирование
позволяет исключить большой класс ошибок-опечаток уже на стадии компиляции. Объявления переменных обычно связывают со статической типизацией, но это не идентичные концепции.
Например в языке Perl есть режим strict vars, при котором каждая переменная должна быть объявлена, а вот значение и тип ей можно назначить значительно позже.

Тем не менее, объявление переменной чаще всего одновременно означает и указание её типа. Именно языки, где тип переменной должен быть задан сразу, называют языками со статической типизацей.
В классических императивных языках, таких как Pascal, статическая типизация затрудняет решение некоторых задач. Например, мы можем сделать бинарное дерево, содержащее строки, и бинарное дерево,
содержащее числа, но нам придётся дублировать практически весь код потому, что в узлах содержатся элементы разного типа.

В 1963 году Джон Алан Робинсон придумал *принцип резолюции*, позволявший эффективно доказывать формализованные теоремы. Этот принцип нашёл своё применение при реализации языков
Prolog и ML. Создавая ML, его авторы, Хиндли и Милнер, опирались на *типизированное λ-исчисление*. Они разработали алгоритм автоматического выведения типов, который сейчас называется
их именами. Алгоритм, используя данные об операциях, где участвуют переменные, накладывает ограничения на множество типов, а затем *унифицирует* их. Унификация в данном контексте
означает *приведение к одному*, то есть в результате работы алгоритма каждая переменная получает свой тип. Важной особенностью алгоритма является возможность работы с параметризованными
типами, то есть в отличии от Pascal, в ML можно описать бинарное дерево чего-нибудь, а не конкретно строк или чисел.

Вот например, бинарное дерево, описанное на языке OCaml:

```ocaml
type 'a binary_tree = Leaf
                    | Node of 'a * 'a binary_tree * 'a binary_tree
```

Элементами бинарного дерева являются либо концевые участки (`Leaf`), либо узлы, содержащие кортеж из элемента любого типа (обозначен `'a`) и двух поддеревьев. В OCaml при описании
кортежа элементы разделяются символом **\***.

Вот так выглядит функцию определения, содержит ли дерево элемент:

```ocaml
let rec contains x t = match t with
                     | Lead -> false
                     | Node (y, left, right) -> x = y || contains x left || contains x right
```

Благодаря выведению типов мы можем не указывать тип параметров `x` и `t`, а также тип возвращаемого значения. Сейчас алгоритмы выведения типов активно используются даже в таких
императивных языках, как C++, Java и C#, правда, из-за особенностей системы типов в этих языках, классический алгоритм Хиндли-Милнера к ним неприменим.

## Логическое программирование



## Назад к истокам

Рубеж 70&ndash;80 годов оказался роковым для программистов. Компьютеры дешевели, росли объёмы оперативной памяти и быстродействие. В этих условиях активно развивались «ресурсоёмкие»
языки, такие как Smalltalk. Появление персоналок вернуло индустрию на 10 лет назад, в ситуацию, где битва шла за каждый килобайт.

Smalltalk для первых Atari, Apple и IBM PC оказался слишком требовательным. Впрочем, как и основные функциональные языки программирования. Одним из важных ограничений функциональной
экосистемы является практическая обязательность сборщика мусора. Другим&nbsp;&mdash; неизменяемость данных и необходимость их копирования. Всё это оказалось неподъёмной задачей для
маломощных машин начала 80-х.

Но история шла своим чередом. В 1983г. молодой (тогда ещё) программист Бьярн Страустрап, размышляя об эффективной реализации полиморфизма, придумал простой и элегантный способ
переопределения методов через таблицу виртуальных функций. Способ оказался настолько хорош, что используется сейчас не только в C++, но и в подавляющем большинстве современных
объектно-ориентированных языков. Приблизительно в начале 90-х годов наступила эпоха объектно-ориентированного программирования.

Объектная парадигма упрощает решение двух важных задач, возникающих в индустрии. Одна из них касается повторного использования кода, а вторая&nbsp;&mdash; совместной
разработки программ. Решения, конечно, предлагались и раньше (процедурное и модульное программирование), но они переставали работать на сложных проектах.

Наверное, настало время отметить, что каждая новая парадигма не отменяла предыдущую, а, скорее, развивала. Структурный Паскаль, оставался таким же императивным языком, как и Фортран,
а объектно-ориентированная Java&nbsp;&mdash; такой же структурной, как и Модула-2. И именно потому, что в Java нет оператора GOTO, средства анализа программ могут отследить
поток управления и выдавать предупреждения о недостижимом коде или вычислять процент покрытия тестами.

Каждый «переход» делал код менее эффективным, но более удобным для написания и сопровождения. Потерю производительности нивелировала возрастающая мощность компьютеров.
В 1995-м году появился новый язык программирования&nbsp;&mdash; Java, призванный решить основные проблемы C++, накопившиеся к тому времени. С моей точки зрения, самыми важными
нововведениями стали виртуальная машина и сборщик мусора, и чуть ниже я расскажу, почему. Как всегда, основной претензией к первым версиям Java стала низкая производительность
(по сравнению с C++). Впрочем, уже тогда было понятно, что существует большой класс задач, для которых этой производительности хватает.

Рост мощностей компьютеров и планомерная работа авторов языка над виртуальной машиной и сборщиком мусора привели к ожидаемому результату&nbsp; ныне Java фактически вытеснила Си++ из
прикладного программирования. Период с 2000г., когда Java получила повсеместное признание и по наши дни, можно считать новой эпохой, эпохой высокоуровневого объектного программирования.
Как обычно, круглая дата является приблизительной, поскольку не стоит относиться к ней с абсолютной серьёзностью. Точно также не стоит серьёзно воспринимать пафосную фразу о
«высокоуровневом объектном программировании»&nbsp;&mdash; она, скорее, свидетельствует о моём неумении подобрать более человечное название.

Язык C++ оказался очень сложным из-за того, что решал одновременно множество задач (совместимость с C, высокая эффективность, поддержка обобщённого и объектного программирования).
Разработчики Java, отринув наследие 70-х, сумели создать доступный C++, между простой и эффективностью выбирая простоту.

И проложили дорогу функциональным языкам.

## Назад в будущее