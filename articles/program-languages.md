---
title: Языки программирования
excerpt: Сакрально-ироническое введение в историю и перспективы.
id: program-languages
---

### Кризис оснований математики

В основу современного программирования легли несколько математических работ, написанных в 20&ndash;30-х годах XX века. Наша профессия возникла как побочный
эффект битвы математиков за математику.

Битва эта началась в конце XIX века, когда разные части математики удалось свести в единую систему. С помощью *теории множеств* учёные обосновали и алгебру, и геометрию, и исчисление предикатов.

Недолгую идилию разрушил Бертран Рассел. Он сформулировал парадокс, доказавший противоречивость теории множеств. И, поскольку из неё следует
даже элементарная арифметика, перед математиками встал вопрос, не противоречива ли и вся математика?

> #### Парадокс Рассела
> Предположим, у нас есть множество всех множеств, которые не являются собственными элементами. Является ли это множество собственным элементом?
> * Если да, то оно не может входить в множество всех множеств, которые не являются собственными элементами, и, таким образом, не является собственным элементом.
> * Если нет, то оно входит в множество всех множеств, которые не являются собственными элементами, и, таким образом, является собственным элементом.

Формулировка выглядит шутливой, однако именно парадокс Рассела привёл к *кризису оснований математики*.

Причина парадокса в том, что классическая (наивная) теория множеств не слишком формально определяет *множества всех множеств* и
*множества, которые являются собственными элементами*. Математики, в том числе и Рассел, предложили несколько способов решения проблемы, и, в конце концов,
теория устояла.

Однако новый неожиданный вопрос так и остался без ответа: как доказать непротиворечивость математики?

> #### Непротиворечивость
> Стоит ли вообще городить огород? Так ли плоха противоречивость? И, кстати, что это такое?
>
> Противоречивость это свойство теории. Теорию называют противоречивой, если в ней истинны и утверждение, и его отрицание. Математически это можно записать как
> **(p&nbsp;&amp;&nbsp;&not;p)&nbsp;&rArr;&nbsp;q**. Из утверждения **p** и его отрицания **&not;p** следует утверждение **q**. По закону исключения третьего
> **(p&nbsp;&amp;&nbsp;&not;p)** всегда **false**, так что мы можем записать **false&nbsp;&rArr;&nbsp;q**.
>
> По правилам **импликации** из ложной посылки могут следовать как истинные, так и ложные следствия, то есть **q** может быть любым. Иначе говоря, если теория
> противоречива, из неё можно вывести всё, что угодно.
>
> Практическая польза теорий заключается в том, что они позволяют делать достоверные предсказания. Если теория предсказывает противоположные исходы, она бесполезна.

За доказательство непротиворечивости взялся *предводитель математиков* Давид Гильберт. Он формализовал понятия *теории* и *доказательства*, и предложил исследовать их математическими средствами.

Учёным удалось формально описать элементарную логику, логику предикатов, арифметику и теорию множеств. Каждую из этих теорий можно представить, как расширение предыдущей, например,
исчисление предикатов это исчисление высказываний с *предикатами* и *кванторами*, а арифметика, в свою очередь&nbsp;&mdash; исчисление предикатов с *нулём*, операцией *следующе число*,
и операцией *сложение*.

Будучи авторами теории, мы могли бы задаться такими вопросами:

1. Достаточно ли у нас аксиом для того чтобы вывести все важные теоремы? *Полон* ли набор аксиом?

1. Нет ли среди аксиом таких, которые дублируют друг друга? *Независимы* ли аксиомы?

1. Не противоречат ли правила вывода друг другу? *Непротиворечива* ли теория?

1. Какова сложность теории? Можно ли предложить способ поиска доказательств? Если нет, можно ли *разрешить вопрос*, является теорема доказумой, или нет?

Для самой простой из теорий&nbsp;&mdash; исчисления высказываний&nbsp;&mdash; можно доказать и полноту, и непротиворечивость. Алгоритм *разрешения* в элементарной
логике также существует. Например, если мы хотим доказать *закон контрапозиции* **(x&nbsp;&rArr;&nbsp;y)&nbsp;&rArr;&nbsp;(&not;y&nbsp;&rArr;&nbsp;&not;x)**, нам достаточно
построить таблицу истинности:

| **x** | **y** | **u = x &rArr; y** | **v = &not;y &rArr; &not;x** | **u &rArr; v** |
|------:|------:|-------------------:|-----------------------------:|---------------:|
|   0   |   0   |                  1 |                            1 |              1 |
|   0   |   1   |                  1 |                            1 |              1 |
|   1   |   0   |                  0 |                            0 |              1 |
|   1   |   1   |                  1 |                            1 |              1 |

Поскольку в крайней правой колонке всегда 1, теорема истинна, хотя мы и не можем предложить метода доказательства, кроме полного перебора.

Следующая по сложности теория&nbsp;&mdash; исчисление предикатов&nbsp;&mdash; также полна и непротиворечива, как и арифметика с операцией сложения.

А вот арифметика со сложением и умножением уже принципиально неполна, независимо от количества аксиом. Неполны и все теории, которые включают арифметику, например, теория множеств.

Это положение доказал Курт Гёдель. Его первая теорема о неполноте гласит, что любая формальная теория, не менее сложная, чем арифметика, либо полна, либо непротиворечива. Вторая теорема утверждает,
что средствами арифметики нельзя доказать её собственную непротиворечивость.

Следует ли из этого, что мы доказали противоречивость арифметики? Нет. Ученик Гильберта немецкий математик Генцен, доказал непротиворечивость арифметики, применив более мощную теорию.
Но её непротиворечивость также требовала обоснования. В полном соответствии с теоремами Гёделя, надеждам на обоснование математики не суждено было сбыться.

Но, если математики проиграли сражение за полноту и непротиворечивость, может быть они могли выиграть на поле разрешимости?

### Проблема разрешения

Как мы помним, в исчеслении высказываний существует *разрешаюшая процедура* для определения истинности теорем. Чтобы узнать, верна ли теорема, мы строим таблицу истинности и изучаем
столбец результата. Жизнь математиков сильно упростилась бы, умей они определять истинность теорем во всех системах.

Уже известный нам Давид Гильберт сформулировал *проблему разрешения*&nbsp;&mdash; найти *разрешающую процедуру* для систем сложнее логики.

*Проблема разрешения* (*нем.* Entscheidungsproblem, *произносится* ɛntˈʃʌɪdʊŋsˌpɹɒbləm) важна тем, что вводит понятие *процедуры* или *алгоритма*. Чтобы разобраться с ней,
американский математик Алонзо Чёрч применил разработанное им лябда-исчисление.

> #### Лямбда-исчисление
> Чёрч исследовал природу функций, и формализовал такие понятия, как **функция**, **параметр**, **применение функции** и **связывание переменной**.
>
> Что можно сделать с помощью таких несложных конструкций? Чёрчу удалось представить в виде λ-функций логику и арифметику
> натуральных чисел вместе с операциями конъюнкции, дизъюнкции, сложения и умножения (ищите в сети **булеаны Чёрча** и **нумералы Чёрча**).
>
> **Вычислимыми** считаются функции, которые можно представить в виде λ-выражения, например, сложение.

Вычислима ли разрешающая процедура для логики предикатов? В своей работе Чёрч доказал, что нет. Следовательно, *проблема разрешения*
не может быть решена ни для неё, ни для более сложных систем.

С другой стороны, некоторые функции вполне себе λ-определимы, и, следовательно, у нас есть способ их вычисления.

Через несколько месяцев после Чёрча неразрешимость Entscheidungsproblem доказал английский математик Алан Тьюринг, использовав умозрительную конструкцию, которая сейчас называется *машиной Тьюринга*.

> #### Машина Тьюринга
> Машина похожа на обычную печатную машинку, только количество кареток в ней не ограничено. Каждая каретка работает со своей лентой, которая бесконечна в обе стороны.
>
> Машина умеет выполнять команды: напечать символ, стереть символ, сдвинуть каретку вправо или влево. В &laquo;программе&raquo; Тьюринга все команды пронумерованы,
> но концепции &laquo;последовательного исполнения&raquo; там нет&nbsp;&mdash; в конце каждой команды надо указать, какую команду выполнить следующей.

Что такое вычислимость по Тьюрингу? Возьмём машину из трёх лент и на первых двух лентах запишем два числа в унарной системе счисления.
Единицу обозначим символом <span style="border: 1px solid black">X</span>, двойку&nbsp;&mdash; символами <span style="border: 1px solid black">X</span><span style="border: 1px solid black">X</span>
в двух соседних клетках, тройку&nbsp;&mdash; <span style="border: 1px solid black">X</span><span style="border: 1px solid black">X</span><span style="border: 1px solid black">X</span>,
и так далее. Последнюю ленту оставим пустой.

Сможем ли мы составить программу, которая напечатает на третьей ленте сумму чисел с первых двух лент? Если нам это удастся (подсказка: нам удастся), то сложение
*вычислимо по Тьюрингу*.

Тьюринг доказал, что нельзя составить разрешающую процедуру для арифметики, как программу для его машины. Так как его работа появилась позже работы Чёрча, он
сравнил два доказательства. Оказалось, что с помощью λ-исчисления и машины Тьюринга можно вычислять одни и те же функции.
Как сказал бы математик, классы вычислимых функций совпадают. Для программиста это означает, что любую функциональную программу, можно 
написать на императивном языке, а любую императивную&nbsp;&mdash; на функциональном.

### Тьюринг-полные языки программирования

Если λ-исчисление и машина Тьюринга эквиваленты, означает ли это, что экивалентны все известные *модели вычисления*? Нет.

Языки, не менее мощные, чем машина Тьюрига, называются полными по Тьюрингу. Существуют и неполные языки, например, *регулярные выражения*.
Предположим, мы хотим написать программу, которая проверяет правильность арифметической формулы. Одним из правил для проверки
будет парность скобок. В регулярных выражениях не существует способа проверить, что количество левых скобок равно количеству правых, кроме
как явно перечислить все случаи:

    ε | (ε) | ((ε)) | (((ε))) | …

Для бесконечной формулы нам потребуется написать бесконечное регулярное выражение, в то время как *контекство-свободная грамматика*
позволяет обойтись короткой программой:

    ε ::= (ε)

Если говорить об эквивалентности, то регулярные выражения совпадают с конечными автоматами, а контекстно-свободные грамматики&nbsp;&mdash; со
стековой машиной (конечный автомат с одним стеком).

Но и стековая машина не является Тьюринг-полной. Предположим, мы считаем правильными такие строки:

    αβγ
    ααββγγ
    αααβββγγγ
    α…αβ…βγ…γ
     ͫ  ͫ  ͫ

Количество символов `α`, `β` и `γ` должно совпадать. Стековая машина, в отличие от привычных языков программирования,
не позволяет распознавать такое множество строк. Но стоит добавить к ней второй стек, как она становится эквивалентна машине Тьюринга.

На практике, от языков программирования не требуется быть Тьюринг-полными. Например, один из самых популярных&nbsp;&mdash; C&nbsp;&mdash; не
является полным из-за *адресной арифметики*. Указатель в C может быть преобразован к целому числу, а это означает, что память C-машины потенциально
ограничена. В то же самое время, в Java и C# указатель это просто указатель, стандарты языков не накладывают ограничений на его размер. Используя
потенциально бесконечную структуру данных *однонапралвенный список*, мы можем описать объект любого размера. А вот массивы в Java и C# имеют
целочисленный размер, поэтому не могут являться основой для Тьюринг-полной программы.

Мы можем делать выводы об языке программирования, зная, является ли он Тьюринг-полным. Скажем, классический SQL не поддерживает рекурсию,
поэтому не позволяет извлекать иерархические данные произвольной вложенности. С другой стороны, SQL с поддержкой обобщённых
табличных выражений (common table expressions)&nbsp;&mdash; позволяет.

Препроцессор C не реализует циклы, поэтому с его помощью нельзя вычислять числа Фибоначчи на этапе компиляции. А с помьщью Тьюринг-полных шаблонов C++&nbsp;&mdash; можно.

Модели вычисления играют важную роль, если в расчёт принимаются временная сложность алгоритма и простота его реализации. Примитивные вычислители проще в разработке и быстрее.
Именно поэтому для создания компиляторов в UNIX используют две программы: **Lex** и **Yacc**. Первая умеет расознавать лексемы, и по сложности соответствует конечному автомату.
Вторая разбирает бесконечно вложенные выражения и соответствует стековой машине. Мы могли бы ограничиться одной программой **Yacc**, поскольку она умеет то же, что
и **Lex**. Но раз стековая машина работает медленнее, компилятор, сделанный только на **Yacc**, тоже окажется медленнее.

Все программисты знают, как иногда мучительны медленные компиляторы.

### Первые языки программирования

Историю языков программирования можно разложить в простую схему, которая, как и все простые схемы, будет неверной. Например, можно сказать, что первая веха в становлении
программирования&nbsp;&mdash; период с 45-го по 55-й год. Удобно, что числа круглые.

На самом деле, писать первые программы начали чуть раньше 45-го года, в 43-м. Средством общения с компьютером были *машинные коды*. *Язык ассемблера* появился в 49-м году.
Веха закончилась созданием первого высокоуровневого языка программирования *Фортран* в 54-м году.

«Веха 43&ndash;54» выглядит совершенно некругло. И год окончания не совсем корректен&nbsp;&mdash; на языке ассемблера массово писали даже в 80-х, так что появление Фортрана,
если и изменило ситуацию, то не сразу, и не быстро. Так что говорить мы будем не о вехах, а о тенденциях.

Архитектура первых компьютеров была основана на модели Тьюринга. Собрать машину из электронных компонентов того времени оказалось проще, чем λ-вычислитель.
Аналогом ленты в компьютере служит оперативная память. Она состоит из ячеек, куда можно записать число из небольшого диапазона, или, как сказали бы
математики, символ из ограниченного алфавита.

Первую память делали десятичной, и, как результат, достаточно сложной. Программы кодировались перемечками на специальной панели, что было неудобно.
Уже к 46-му году разработчики придумали, как упростить схему вычислительных машин. Набор принципов был изложен Джоном фон Нейманом, и известен как *архитектура фон Неймана*:

1. В одной и той же памяти хранится как данные, так и команды.
1. Память компьютера пронумерована, номер или адрес ячейки позволяет процессору прочитать или изменить содержимое ячейки.
1. Процессор интерпретирует программу, как набор команд в оперативной памяти. Он выполняет их последовательно, пока не встретит команду перехода.
1. Вся информация кодируется в двоичной системе.

Машинный код&nbsp;&mdash; программа в том виде, как её видит процессор, то есть набор двоичных слов, каждое из которых либо *код команды*, либо её *параметры*.
Такая программа *императивна*, поскольку содержит набор инструкций (указаний, императивов), которые машине следует выполнить.

Первым шагом на пути упрощения работы программистов стал язык Ассемблера. Важная терминологияеская тонкость заключается в том, что Ассемблер&nbsp;&mdash; это программа,
переводящая мнемоники в машинный код. Будучи *настоящим программистом*, нельзя говорить &laquo;написал программу на Ассемблере&raquo;&nbsp;&mdash; только &laquo;на
языке Ассемблера&raquo;.

В языке Ассемблера все машинные команды кодируются простыми словами, зачастую даже сокращениями: `MOV` (move), `DIV` (division), `XCHG` (exchange) и даже `JNE`
(jump if not equal). Кроме того, Ассемблер во время трансляции самостоятельно вычисляет относительные адреса меток и процедур и избавляет от этой работы программиста.
В результате, мы можем осуществлять переход на именованную метку или вызывать именованную процедуру.

Чтобы почувствовать вкус языка Ассемблера, приведу код программы для виртуальной машины LLVM, которая печатает первые двадцать чисел Фибоначчи.

```llvm
@format = private constant [3 x i8] c"%d\0A"

define i32 @main() {
entry:
  %a = alloca i32, align 4
  store i32 0, i32* %a
  %b = alloca i32, align 4
  store i32 1, i32* %b

  %i = alloca i32, align 4
  store i32 0, i32* %i

  br label %l1

l1:
  %0 = load i32* %a, align 4

  %1 = call i32 (i8*, ...)* @printf(i8* getelementptr inbounds ([3 x i8]* @format, i32 0, i32 0), i32 %0)

  %2 = load i32* %b, align 4
  %3 = add i32 %0, %2
  store i32 %3, i32* %a
  store i32 %0, i32* %b

  %4 = load i32* %i, align 4
  %5 = add i32 %4, 1
  store i32 %5, i32* %i

  %6 = icmp eq i32 %5, 20
  br i1 %6, label %l2, label %l1
  
l2:
  ret i32 0
}

declare i32 @printf(i8*, ...)
```

Не вдаваясь в детали, можно заметить, что программист делает много работы низкого уровня: самостоятельно распределяет локальную память процедуры, кодирует вычисления, организует
цикл с помощью инструкций `IF` и `GO TO`. Для печати чисел программа вызывает функцию `printf` системной библиотеки. На самом деле это старая добрая функция `printf` из C:

```c
printf("%d\n", a);
```

Функция возвращает целое число&nbsp;&mdash; количество напечатанных символов. Результат работы `printf` обычно игнорируется в C, но в языке Ассемблера
мы не можем просто так его отбросить: мы явно сохраняем результат в регистре, а потом его не используем.

На языке Ассемблера сложно записывать математические выражения, такие как **SQRT(X&nbsp;+&nbsp;Y&nbsp;\*&nbsp;38.7)**. Проблему перевода выражений в машинный код решил Фортран, само название
которого означает Formula Translator. Также в Фортране появился оператор цикла `DO`, который не имеет аналога в машинном коде. Он собирается из примитивов `IF` и `GO TO`,
и является, по сути, синтаксическим сахаром. Через 15 лет (почти через 15) он позволил сформулировать принципы *структурного программирования*.

Наконец, Фортран стандартизировал вызов процедур. Программируя на машинном языке, мы можем вызывать подпрограммы, самостоятельно организуя передачу параметров и возврат значений через регистры,
стек, кучу и глобальную память. Даже в пределах одной программы допустимо произвольным образом смешивать разные способы вызова. Однако появление компилятора привело к необходимости формализовать
*соглашение о вызовах*. И если разные компиляторы следуют одному соглашению, можно стыковать подпрограммы, написанные на разных языках. Благодаря этому код на Фортране до сих пор легко подключить
к проектам, написанным на C, C++ и даже Java.

Не смотря на то, что Фортран является императивным языком, запись математических выражений на нём функциональна. Увидев формулу **A&nbsp;\*&nbsp;B&nbsp;+&nbsp;A&nbsp;\*&nbsp;C**,
мы не можем предсказать, в каком порядке будут вычисляться слагаемые: сначала **A&nbsp;\*&nbsp;B** или сначала **A&nbsp;\*&nbsp;C**. Истинно императивной записью выражений является
постфиксная форма, которая обязывает программиста думать о порядке выполнения операций: **A&nbsp;B&nbsp;\*&nbsp;A&nbsp;C&nbsp;\*&nbsp;+**. При такой записи, как видим, разночтений нет.

Напечатаем первые двадцать чисел Фибоначчи на Фортране.

```fortran
program fibonacci
  integer a, b, t
  a = 0
  b = 1
   
  do n = 1,20
    print *, a
    t = a
    a = a + b
    b = t
  end do
end program fibonacci
```

Как и в программе на языке Ассемблера, мы используем аккумуляторы `a` и `b`, которые содержат два последних вычисленных значения.
Временная переменная `t` позволят обменивать значения `a` и `b` перед тем, как вычислить следующее число. Благордаря этому обмену мы сохраняем
семантику &laquo;`a`&nbsp;&mdash; последнее вычисленное число, а `b`&nbsp;&mdash; предпоследнее&raquo;.

Некоторые программисты любят &laquo;оптимизировать&raquo; код, чтобы отказаться от переменной `t`,

```fortran
    print *, a
    a = a + b
    b = a - b
```

На мой взгляд, этот код хуже, потому что требует больших усилий при чтении. Здесь не очевидно, что речь идёт об обмене значений.
В коде есть ещё одна тонкость: перед запуском цикла `a` хранит предпоследнее число Фибоначчи, а `b`&nbsp;&mdash; последнее. После
первой итерации ситуация исправляется. Этот маленький трюк позволяет упростить цикл.

Результат работы:

```
     0
     1
     1
     2
     3
     5
     8
    13
    21
    34
    55
    89
   144
   233
   377
   610
   987
  1597
  2584
  4181
```

### Функциональное программирование

ЛИСП, дедушка современных функциональных языков, был разработан в 1958-м году, всего через четыре года после Фортрана.
Его название расшифровывается как list processor, то есть *обработчик списков*.

Очевидно, *список*&nbsp;&mdash; одно из фундаментальных понятий в языке. Речь идёт о связном однонаправленном списке,
где вставка в начало выполняется за время **O(1)**, то есть очень быстро. Эта структура данных настолько удобна, что
перекочевала во все современные функциональные языки.

Данные и программы записываются одинаково, в виде S-выражений, знаменитой скобочной записи:

```common-lisp
(cons 1 '(2 3 4))
```

Здесь мы вызываем функцию `cons` с параметрами `1` и `(2 3 4)`, то есть одиночным числом
и списком чисел. Функция возвращает список, где первый параметр вставлен в начало второго, т.е. `(1 2 3 4)`.

Такая запись подчёркивает факт, что процедуры ничем не отличаются от данных: функция может принимать
функцию в качестве параметра и возвращать её как результат. Единственное отличие заключается в том, что функцию
можно *вызвать* или, как говорят математики, *применить*.

Напечатаем первые двадцать чисел Фибоначчи на Scheme, современном наследнике языка ЛИСП:

```common-lisp
(define (fibonacci-series n)
 (define (build-fibonacci m a b l)
  (cond
   ( (= m 0) l)
   (else (build-fibonacci (- m 1) (+ a b) a (cons a l)))))
  (build-fibonacci n 0 1 '()))

(print (fibonacci-series 20))
``` 

Результат:

```common-lisp
(4181 2584 1597 987 610 377 233 144 89 55 34 21 13 8 5 3 2 1 1 0)
```

Здесь мы написали функцию `fibonacci-series`, которая принимает в качестве параметра размер последовательности. В результате её
вызова мы получаем список чисел Фибоначчи указанной длины.

Для того, чтобы строить список эффективно, мы применяем *хвостовую рекурсию* во вложенной процедуре `build-fibonacci`.
Каждое следующее число Фибоначчи добавляется в начало списка&nbsp;&mdash; как мы знаем, это очень быстро. Поэтому числа в нашем
списке расположены в порядке убывания.

## Структурное программирование

Конец эпохи Фортрана принято связывать со знаменитой статьёй Дейкстры «О вреде оператора GOTO», которая вышла в 1968г. В данном случае слова «конец эпохи» следует воспринимать условно,
поскольку в математических программах Фортран активно используется до сих пор.

В действительности основы структурного подхода были заложены за десять лет до статьи, при разработке языка программирования Algol-58. Если Fortran следовал той же парадигме
программирования, что и машинный код&nbsp;&mdash; программа это просто набор команд, то в Algol ввели понятие *структуры*. Программа разбивалась на *блоки*, которые могли быть вложенными.
Плюс такого разбиения заключается в снижении количества ошибок. При программировании на Fortran бичом программистов был так называемый *спагетти-код*, в котором сложно отследить порядок
выполнения, поскольку управление постоянно скачет то вверх, то вниз.

Дейкстра предложил несколько принципов, которые позволили читать подпрограмму сверху вниз. Он доказал, что любой спагетти-код можно преобразовать в последовательность одиночных операторов,
циклов и ветвлений. В современных языках оператор `GO TO` практически не встречается, вместо него используют такие операторы как `break`, `continue` и `return`.

Структура программ позволила разрабатывать средства для работы с *потоком управления*. Например, компилятор может предупредить программиста о *недостежимом коде*, потому что знает,
что в определённые места программы управление никогда не попадёт.

Посмотрим, как задача с числами Фибоначчи решается в языке структурного программирования Паскаль:

```pascal
program FibonacciSeries;

var
  A, B, T: integer;
  I: integer;

begin
  A := 0;
  B := 1;
  
  for I := 1 to 20 do
  begin
    WriteLn(A);
    T := A;
    A := A + B;
    B := T;
  end;
end.
```

Здесь мы видим блоки, ограниченные ключевыми словами `begin` и `end`, а также отступы, которые структуру. В Паскале количество пробелов в начале строки не значимо,
но хорошей практикой считается &laquo;сдвигать&raquo; вложенные блоки вправо.

## Типы данных

Fortran умел работать с переменными двух типов: целых и действительных чисел. Переменные можно было не описывать, поскольку в языке применялись правила автоматического объявления типа.
Переменные, чьё имя начиналось на `I`, `J`, `K`, `L`, `M` и `N` считались целыми, все прочие&nbsp;&mdash; действительными.

При разработке Algol объявление переменных вместе с указанием их типа сделали явным. В программе необходимо указать название каждой используемой переменной и её тип. Такое дублирование
позволяет исключить большой класс ошибок-опечаток уже на стадии компиляции. Объявления переменных обычно связывают со статической типизацией, но это не идентичные концепции.
Например в языке Perl есть режим strict vars, при котором каждая переменная должна быть объявлена, а вот значение и тип ей можно назначить значительно позже.

Тем не менее, объявление переменной чаще всего одновременно означает и указание её типа. Именно языки, где тип переменной должен быть задан сразу, называют языками со статической типизацей.
В классических императивных языках, таких как Паскаль, статическая типизация затрудняет решение некоторых задач. Например, мы можем сделать бинарное дерево, содержащее строки, и бинарное дерево,
содержащее числа, но нам придётся дублировать практически весь код потому, что в узлах содержатся элементы разного типа.

```pascal
type
  IntegerBinaryNode = record
    Value: Integer;
    Left: ^IntegerBinaryNode;
    Right: ^IntegerBinaryNode;
  end;

  StringBinaryNode = record
    Value: String;
    Left: ^StringBinaryNode;
    Right: ^StringBinaryNode;
  end;
```

В 1963 году Джон Алан Робинсон придумал *принцип резолюции*, позволявший эффективно доказывать формализованные теоремы. Этот принцип нашёл своё применение при реализации языков
Prolog и ML. Создавая ML, его авторы, Хиндли и Милнер, опирались на *типизированное λ-исчисление*. Они разработали алгоритм автоматического выведения типов, который сейчас называется
их именами. Алгоритм, используя данные об операциях, где участвуют переменные, накладывает ограничения на множество типов, до тех пор, пока не *унифицирует* их. Унификация в данном контексте
означает *приведение к одному*, то есть в результате работы алгоритма каждая переменная получает свой тип. Важной особенностью алгоритма является умение работать с параметризованными
типами, то есть в отличии от Pascal, в ML можно описать бинарное дерево чего-нибудь, а не конкретно строк или чисел.

Вот например, бинарное дерево, описанное на языке OCaml:

```ocaml
type 'a binary_tree = Leaf
                    | Node of 'a * 'a binary_tree * 'a binary_tree
```

Элементами бинарного дерева являются либо концевые участки (`Leaf`), либо узлы, содержащие кортеж из элемента любого типа (обозначен `'a`) и двух поддеревьев. В OCaml при описании
кортежа элементы разделяются символом **\***.

Вот так выглядит функцию определения, содержит ли дерево элемент:

```ocaml
let rec contains x t = match t with
                     | Lead -> false
                     | Node (y, left, right) -> x = y || contains x left || contains x right
```

Благодаря выведению типов мы можем не указывать тип параметров `x` и `t`, а также тип возвращаемого значения. Сейчас алгоритмы выведения типов активно используются даже в
императивных языках, например, в C++, Java и C#..

## Логическое программирование



## Назад к истокам

Рубеж 70&ndash;80 годов оказался роковым для программистов. Компьютеры дешевели, росли объёмы оперативной памяти и быстродействие. В этих условиях активно развивались «ресурсоёмкие»
языки, такие как Smalltalk. Появление персоналок вернуло индустрию на 10 лет назад, в ситуацию, где битва шла за каждый килобайт.

Smalltalk для первых Atari, Apple и IBM PC оказался слишком требовательным. Впрочем, как и основные функциональные языки программирования. Одним из важных ограничений функциональной
экосистемы является практическая обязательность сборщика мусора. Другим&nbsp;&mdash; неизменяемость данных и необходимость их копирования. Всё это оказалось неподъёмной задачей для
маломощных машин начала 80-х.

Но история шла своим чередом. В 1983г. молодой (тогда ещё) программист Бьярн Страустрап, размышляя об эффективной реализации полиморфизма, придумал простой и элегантный способ
переопределения методов через таблицу виртуальных функций. Способ оказался настолько хорош, что используется сейчас не только в C++, но и в подавляющем большинстве современных
объектно-ориентированных языков. Приблизительно в начале 90-х годов наступила эпоха объектно-ориентированного программирования.

Объектная парадигма упрощает решение двух важных задач, возникающих в индустрии. Одна из них касается повторного использования кода, а вторая&nbsp;&mdash; совместной
разработки программ. Решения, конечно, предлагались и раньше (процедурное и модульное программирование), но они переставали работать на сложных проектах.

Наверное, настало время отметить, что каждая новая парадигма не отменяла предыдущую, а, скорее, развивала. Структурный Паскаль, оставался таким же императивным языком, как и Фортран,
а объектно-ориентированная Java&nbsp;&mdash; такой же структурной, как и Модула-2. И именно потому, что в Java нет оператора GOTO, средства анализа программ могут отследить
поток управления и выдавать предупреждения о недостижимом коде или вычислять процент покрытия тестами.

Каждый «переход» делал код менее эффективным, но более удобным для написания и сопровождения. Потерю производительности нивелировала возрастающая мощность компьютеров.
В 1995-м году появился новый язык программирования&nbsp;&mdash; Java, призванный решить основные проблемы C++, накопившиеся к тому времени. С моей точки зрения, самыми важными
нововведениями стали виртуальная машина и сборщик мусора, и чуть ниже я расскажу, почему. Как всегда, основной претензией к первым версиям Java стала низкая производительность
(по сравнению с C++). Впрочем, уже тогда было понятно, что существует большой класс задач, для которых этой производительности хватает.

Рост мощностей компьютеров и планомерная работа авторов языка над виртуальной машиной и сборщиком мусора привели к ожидаемому результату&nbsp; ныне Java фактически вытеснила Си++ из
прикладного программирования. Период с 2000г., когда Java получила повсеместное признание и по наши дни, можно считать новой эпохой, эпохой высокоуровневого объектного программирования.
Как обычно, круглая дата является приблизительной, поскольку не стоит относиться к ней с абсолютной серьёзностью. Точно также не стоит серьёзно воспринимать пафосную фразу о
«высокоуровневом объектном программировании»&nbsp;&mdash; она, скорее, свидетельствует о моём неумении подобрать более человечное название.

Язык C++ оказался очень сложным из-за того, что решал одновременно множество задач (совместимость с C, высокая эффективность, поддержка обобщённого и объектного программирования).
Разработчики Java, отринув наследие 70-х, сумели создать доступный C++, между простой и эффективностью выбирая простоту.

И проложили дорогу функциональным языкам.

## Назад в будущее