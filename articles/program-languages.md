---
title: Языки программирования
subtitle: Сакрально-ироническое введение в историю и перспективы
id: program-languages
---

## Кризис оснований математики

В основу современного программирования легли несколько математических работ, написанных в 20&ndash;30-х годах XX века. Наша профессия возникла как побочный
эффект битвы математиков за математику.

Битва эта началась в конце XIX века, когда разные части математики удалось свести в единую систему. С помощью *теории множеств* учёные обосновали
и алгебру, и геометрию, и исчисление предикатов.

Недолгую идилию разрушил Бертран Рассел. Он сформулировал парадокс, доказавший противоречивость теории множеств. И, поскольку из неё следует
даже элементарная арифметика, встаёт вопрос, не противоречива ли и она?

> #### Парадокс Рассела
> Предположим, что у нас есть множество всех множеств, которые не являются собственными элементами. Является ли это множество собственным элементом?
> * Если да, то оно не может входить в множество всех множеств, которые не являются собственными элементами, и, таким образом, не является собственным элементом.
> * Если нет, то оно входит в множество всех множеств, которые не являются собственными элементами, и, таким образом, является собственным элементом.
> Формулировка выглядит шутливой, однако именно парадокс Рассела привёл к *кризису оснований математики*.

Причина парадокса в том, что классическая (наивная) теория множеств не слишком формально определяет *множества всех множеств* и
*множества, которые являются собственными элементами*. Математики, в том числе и Рассел, предложили несколько способов решения проблемы, и, в конце концов
теория множеств всё-таки устояла.

Однако, новый неожиданный вопрос так и остался без ответа: как доказать непротиворечивость математики?

> #### Непротиворечивость
> Стоит ли вообще городить огород? Так ли плоха противоречивость? И, кстати, что это такое?
>
> Противоречивость это свойство теории. Теорию называют противоречивой, если в ней истинны и утверждение, и его отрицание. Математически это можно записать как
> <nobr>(*p* &amp; &not;*p*) &rArr; *q*</nobr>. Из утверждения *p* и его отрицания &not;*p* следует утверждение *q*. По закону исключения третьего <nobr>(*p* &amp; &not;*p*)
> всегда **false**, так что мы можем записать <nobr>**false** &rArr; *q*</nobr>.
>
> По правилам *импликации* из ложной посылки могут следовать как истинные, так и ложные следствия, то есть *q* может быть любым. Иначе говоря, если теория
> противоречива, из неё можно вывести всё, что угодно.
>
> Практическая польза теорий проста: они позволяют делать достоверные предсказания. Если теория предсказывает противоположные исходы, она бесполезна.

За доказательство непротиворечивости взялся *предводитель математиков* Давид Гильберт. Он формализовал понятия *теории* и *доказательства*, и
предложил исследовать их математическими средствами.

Учёным удалось формально описать элементарную логику, логику предикатов, арифметику и теорию множеств. Каждую из этих теорий можно представить,
как расширение предыдущей, например, исчисление предикатов это исчисление высказываний с предикатами и кванторами, а арифметика, в свою очередь, расширяет исчисление предикатов.

Будучи авторами теории, мы могли бы задаться такими вопросами:

1. Достаточно ли у нас аксиом для того чтобы выести все важные теоремы? *Полон* ли набор аксиом?

1. Нет ли среди аксиом тех, которые дублируют друг друга? *Независимы* ли аксиомы?

1. Не противоречат ли правила вывода друг другу? *Непротиворечива* ли теория?

1. Какова сложность теории? Можно ли предложить способ поиска доказательств? Если нет, можно ли *разрешить вопрос*, является теорема доказумой, или нет?

Для самой простой из теорий&nbsp;&mdash; исчисления высказываний&nbsp;&mdash; можно доказать и полноту, и непротиворечивость. Алгоритм *разрешения* в элементарной
логике также существует. Например, если мы хотим доказать *закон контрапозиции* <nobr>(*x* &rArr; *y*) &rArr; (&not;*y* &rArr; &not;*x*)</nobr>, нам достаточно
составить таблицу истинности:

| *x* | *y* | *u* = *x* &rArr; *y* | *v* = &not;*y* &rArr; &not;*x* | *u* &rArr; *v* |
|-----|-----|-------------------------------------------------------|----------------|
|  0  |  0  |                    1 |                              1 |              1 |
|  0  |  1  |                    1 |                              1 |              1 |
|  1  |  0  |                    0 |                              0 |              1 |
|  1  |  1  |                    1 |                              1 |              1 |

Поскольку в крайней правой колонке всегда 1, теорема истинна, хотя мы и не можем предложить метода доказательства, кроме полного перебора.

Следующая по сложности теория&nbsp&mdash; исчисление предикатов&nbsp;&mdash; также полна и непротиворечива, как и арифметика с операцией сложения.

А вот арифметика со сложением и умножением уже принципиально неполна, независимо от количества аксиом. Неполны и все теории, которые включают арифметику, например, теория множеств.

Это положение доказал Курт Гёдель. Его первая теорема о неполноте гласит, что любая формальная теория, не менее сложная, чем арифметика, либо полна, либо непротиворечива.
Вторая теорема утверждает, что средствами арифметики нельзя доказать её собственную непротиворечивость.

Следует ли из этого, что мы доказали противоречивость арифметики? Вовсе нет. Ученик Гильберта немецкий математик Генцен, доказал непротиворечивость арифметики, применив более мощную теорию.
Однако, непротиворечивость этой теории также требовала обоснования. В полном соответствии с теоремами Гёделя, надеждам на обоснование математики формальными методами было не суждено сбыться.

Но, если математики проиграли сражение за полноту и непротиворечивость, может быть они могли выиграть на поле разрешимости?

## Пооблема разрешения

Как мы помним, в исчеслении высказываний при помощи таблицы истинности можно установить, является теорема истинной, или нет. Жизнь математиков сильно упростилась бы, если бы
они могли точно также определять истинность теорем для логики предикатов, арифметики и теории множеств. Уже известный нам Давид Гильберт сформулировал *проблему разрешения*,
которая касается как раз этого вопроса.

Итак, имея формальную систему и некоторое утверждение в рамках этой системы, можем ли мы определить алгоритм, который установил бы истинность этого утверждения?

Ответ на этот вопрос дали математики Алонзо Чёрч и Алан Тьюринг.


## Ретроспектива

### 45-55

Историю языков программирования можно разложить в простую схему, которая, как и все простые схемы, ошибочна. Например, можно сказать, что первая веха в становлении программирования&nbsp&mdash;
это период с 45-го по 55-й год. Удобно, что числа круглые, однако писать первые программы начали не позднее 43-го года. Средством общения с компьютером были *машинные коды* и *язык ассемблера*,
который появился к 49-му году. Веха закончилась созданием первого высокоуровневого языка программирования *Фортран* в 54-м году.

«Веха 43&ndash;54» выглядит совершенно некругло. И год окончания не совсем корректен&nbsp;&mdash; на языке ассемблера массово писали даже в 80-х, так что появление Фортрана, если и изменило ситуацию,
то не сразу, и не быстро.

Машинные коды&nbsp;&mdash; это чистый компьютерный язык. Поскольку архитектура компьютеров основана на модели Тьюринга, первые программы были императивными, то есть содержали
последовательность команд. *Алгоритмы* в машине Тьюринга реализуются именно в виде последовательности команд, которые машина исполняет. Таким же императивным языком стал и Фортран.

Его основное преимущество замечательно раскрывается в названии&nbsp;&mdash; Formula Translator. Фортран научился делать одну простую, но очень важную работу&nbsp;&mdash; переводить выражения вида
`SQRT(X + Y * 38.7)` в последовательность машинных команд. В остальном отличие от языка ассемблера было незначительным, даже условный оператор в его привычной форме `if…then…else`
в Фортране отсутствовал. Вместо него программист ставил метки, управление на которые передавалось, если выражение оказывалось меньше, больше или равно нулю.

Впрочем, отмечу ещё два важных нововведения. Во-первых, это оператор цикла DO, который не имел аналога в языке ассемблера. Эта конструкция «собиралась» из IF и GOTO, то есть, по сути, являлась
синтаксическим сахаром. Через 15 лет (почти через 15) она стала одним из столпов структурного программирования.

Во-вторых, это стандартизация вызова процедур. Программируя на машинном языке, мы можем вызывать подпрограммы, самостоятельно организуя передачу параметров и возврат значений через регистры,
стек, кучу и глобальную память. Даже в пределах одной программы допустимо произвольным образом смешивать разные способы вызова. Однако появление компилятора привело к необходимости формализовать
[соглашение о вызовах](https://ru.wikipedia.org/wiki/%D0%A1%D0%BE%D0%B3%D0%BB%D0%B0%D1%88%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BE_%D0%B2%D1%8B%D0%B7%D0%BE%D0%B2%D0%B5). Если разные компиляторы следуют
одному соглашению, можно стыковать подпрограммы, написанные на разных языках, благодаря чему код на Фортране до сих пор легко подключить к вашим проектам, написанным на C, C++ и даже Java.

## 55-70

Следующие 15 лет (ориентировочно 55-70гг.) индустрия спокойно развивалась, накапливая математические библиотеки.

Конец эпохи Фортрана принято связывать со знаменитой статьёй Дейкстры «О вреде оператора GOTO», которая вышла в 1968г. В данном случае слова «конец эпохи» следует воспринимать условно,
поскольку в математических программах Фортран активно используется до сих пор.

Что же случилось? Ничего, если не считать большого количества ошибок. При программировании на Фортране бичом программистов стал так называемый спагетти-код, в котором сложно отследить порядок
выполнения, поскольку управление постоянно скачет то вверх, то вниз.

Дейкстра предложил несколько принципов, которые позволили читать подпрограмму сверху вниз. Он доказал, что любой спагетти-код можно преобразовать в последовательность одиночных операторов,
циклов и ветвлений. Характерно, что структурное программирование поддерживал уже Алгол-60, однако, массовую популярность методология получила только в 70-е годы.

Помимо структуры, в языки программирования стала проникать концепция объявлений, которая позволила исключить большой класс ошибок-опечаток. Объявления переменных связывают (или путают) со
статической типизацией, однако, например, в языке Perl есть режим strict vars, при котором каждая переменная должна быть объявлена, а вот значение ей можно назначить значительно позже.

Структурное программирование довольно долго оставалось главной парадигмой индустрии&nbsp;&mdash; практически 20 лет (70-90гг.), и это не смотря на то, первым объектно-ориентированным языком принято считать Симулу, созданную в 1967г.
Как ни странно, такое продолжительный застой связан в первую очередь с прогрессом в области железа. Удивлены?
Рубеж 70-80 годов оказался роковым для программистов. Компьютеры дешевели, росли объёмы оперативной памяти и быстродействие. В этих условиях активно развивались «ресурсоёмкие» языки — Смолток, различные варианты LISP и ML. Однако появление персоналок вернуло нас на 10 лет назад, в ситуацию, где битва шла за каждый килобайт.
Смолток для первых Atari, Apple и IBM PC оказался слишком требовательным.
Впрочем, история шла своим чередом. В 1983г. молодой (тогда ещё) программист Бьярн Страустрап, размышляя об эффективной реализации полиморфизма, придумал простой и элегантный способ переопределения методов через таблицу виртуальных функций. Способ оказался настолько хорош, что используется сейчас не только в Си++, но и в подавляющем большинстве современных объектно-ориентированных языков. Приблизительно в начале 90-х годов наступила эпоха объектно-ориентированного программирования. Что же в нём хорошего? Объектная парадигма упрощает решение двух важных задач, возникающих в индустрии. Одна из них касается повторного использования кода, а вторая — совместной разработки программ. Решения, конечно, предлагались и раньше (процедурное и модульное программирование), но они переставали работать на сложных проектах.
Наверное, настало время отметить, что каждая новая парадигма не отменяла предыдущую, а, скорее, развивала. Структурный Паскаль, оставался таким же императивным языком, как и Фортран, а объектно-ориентированная Java — такой же структурной, как и Модула-2.
Каждый «переход» делал код менее эффективным, но более удобным для написания и сопровождения. Потерю производительности нивелировала возрастающая мощность компьютеров.
В 1995-м году появился новый язык программирования — Java, призванный решить основные проблемы Си++, накопившиеся к тому времени. С моей точки зрения, самыми важными нововведениями стали виртуальная машина и сборщик мусора, и чуть ниже я расскажу, почему. Как всегда, основной претензией к первым версиям Java стала низкая производительность (по сравнению с Си++). Впрочем, уже тогда было понятно, что существует большой класс задач, для которых этой производительности хватает. Рост мощностей компьютеров и планомерная работа авторов языка над виртуальной машиной и сборщиком мусора привели к ожидаемому результату — ныне Java фактически вытеснила Си++ из прикладного программирования.
Период с 2000г., когда Java получила повсеместное признание и по наши дни, можно считать новой эпохой, эпохой высокоуровневого объектного программирования. Как обычно, круглая дата является приблизительной, поскольку не стоит относиться к ней с абсолютной серьёзностью. Точно также не стоит серьёзно воспринимать пафосную фразу о «высокоуровневом объектном программировании» — она, скорее, свидетельствует о моём неумении подобрать более человечное название.
Язык Си++ оказался очень сложным из-за того, что решал одновременно множество задач (совместимость с Си, высокая эффективность, поддержка обобщённого и объектного программирования). Разработчики Java, отринув наследие 70-х, сумели создать доступный Си++, между простой и эффективностью выбирая простоту.
И проложили дорогу функциональным языкам.

